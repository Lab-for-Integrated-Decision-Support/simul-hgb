---
title: "03_Clinical_Accuracy"
author: "Adam Dziorny"
date: '2022-10-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The purpose of this markdown is to report clinical accuracy by displaying the Error Grid analysis, Cohen's Kappa, and "test" results. 

## Initialize

First we load the necessary packages:

```{r, warning=FALSE}
suppressPackageStartupMessages({
  
  # Data frame manipulation
  require(dplyr)
  
  # Graphics and output
  require(ggplot2)
  require(cowplot)
  
  # Tables
  require(knitr)
  require(kableExtra)
  
  # Error grid point allocation
  require(ptinpoly)

})
```

Ensure the environmental variables are specified:

```{r}
if (Sys.getenv('PICU_LAB_DATA_PATH') == '' |
    Sys.getenv('PICU_LAB_IMG_PATH') == '' |
    Sys.getenv('PICU_LAB_IN_FILE') == '' |
    Sys.getenv('PICU_LAB_SITE_NAME') == '')
  stop('Missing necessary environmental variables - see README.md')
```

Specify the run date:

```{r}
run.date <- '2022-10-05'
```

## Data Input

Load data from the `DATA_PATH` with the associated `IN_FILE`, adding a file separator between them. This should result in loading two data frames: `cohort.df` and `labs.df`.

```{r}
load(
  file = file.path(
    Sys.getenv('PICU_LAB_DATA_PATH'),
    Sys.getenv('PICU_LAB_IN_FILE')
  )
)
```

## Set Parameters

We will utilize several sensitivity analyses in this markdown - these should be identical to the sensitivity indicators in the prior markdown `02_Analytic_Accuracy.Rmd`. No changes should be made to these parameters - only additions for more sensitivity parameters.

```{r}
# The primary cutoff value between collection times (in minutes) to 
# determine "simultaneous"
primary.cutoff <- 15. 

# Sensitivity analysis list
sens.cutoffs <- c(1., 30., 90.)

# Hgb cutoffs

```

## Join

The below join function is a copy of the function used in `02_Analytic_Accuracy.Rmd`. No changes should be made to this version - make changes to the prior version, re-test within that script, and then copy here. After development, this will be moved to a package.

```{r}
#'
#' @title Create Paired Dataset
#' 
#' @description Creates a dataset of paired simultaneous lab values
#'
#' @param labs.df The labs data frame
#' @param cohort.df The cohort data frame, needed for PAT_KEY and DEPT
#' @param PN A two-element list of PROC_NAMEs to join
#' @param time.diff The max time difference (min) between collected times
#' @param CN The COMP_NAME to join [Default: 'Hgb']
#' @param multi.per.pt If FALSE, limit to first result per patient, otherwise 
#'     if TRUE [Default], allow all
#'     
#' @returns The resulting joined data frame
#'
createPairedDataset <- function (labs.df, cohort.df, PN, time.diff, 
                           CN = 'Hgb', multi.per.pt = T) {
  
  # First we filter to remove the non-numeric rows
  filter.df <- 
    labs.df %>%
    dplyr::filter(!is.na(NUM_VAL) & NUM_VAL != 9999999.) %>%
    dplyr::filter(COMP_NAME == CN)
  
  cat(sprintf('Number of component numeric rows in input data frame: %d\n',
              nrow(filter.df)))
  
  # Join to get PAT_KEY and DEPT, used in subsequent filtering
  keyed.df <- 
    dplyr::left_join(
      x = filter.df,
      y = cohort.df %>% 
        dplyr::select(ENC_KEY, PAT_KEY, DEPT),
      by = c('ENC_KEY')
    )
  
  # Now we filter by PN and join to create full data frame
  joined.df <-
    dplyr::inner_join(
      x = keyed.df %>%
        dplyr::filter(PROC_NAME == PN[1]) %>%
        dplyr::select(ENC_KEY, PAT_KEY, ORDER_PROC_KEY, 
                      DEPT, COLLECTED_DT, RESULT_DT, NUM_VAL, AGE_PROC),
      y = keyed.df %>%
        dplyr::filter(PROC_NAME == PN[2]) %>%
        dplyr::select(ENC_KEY, PAT_KEY, ORDER_PROC_KEY,
                      DEPT, COLLECTED_DT, RESULT_DT, NUM_VAL),
      by = c('ENC_KEY', 'PAT_KEY', 'DEPT'),
      suffix = c('.x', '.y')
    ) 
  
  # Join using base R, by column number
  #   [[5]] is PN[1] COLLECTED_DT
  #   [[10]] is PN[2] COLLECTED_DT
  joined.df$COLL_TIME_DIFF_MIN <-
    as.numeric(joined.df[[5]] - joined.df[[10]], units = 'mins')
 
  # Apply the cutoff time
  cutoff.df <- 
    joined.df %>%
    dplyr::filter(abs(COLL_TIME_DIFF_MIN) < time.diff)
  
  cat(sprintf('Number of paired, simultaneous values meeting cutoff: %d\n',
              nrow(cutoff.df)))
    
  # Ensure that each first PROC_NAME order is only used once - meaning that  
  # each ORDER_PROC_KEY.x should be unique
  unique.x.df <- 
    cutoff.df%>%
    dplyr::arrange(ORDER_PROC_KEY.x, COLL_TIME_DIFF_MIN) %>%
    dplyr::group_by(ORDER_PROC_KEY.x) %>%
    dplyr::summarize(
      ORDER_PROC_KEY.y   = first( ORDER_PROC_KEY.y   ),
      DEPT               = first( DEPT               ),
      COLLECTED_DT.x     = first( COLLECTED_DT.x     ),
      RESULT_DT.x        = first( RESULT_DT.x        ),
      NUM_VAL.x          = first( NUM_VAL.x          ),
      COLLECTED_DT.y     = first( COLLECTED_DT.y     ),
      RESULT_DT.y        = first( RESULT_DT.y        ),
      NUM_VAL.y          = first( NUM_VAL.y          ),
      COLL_TIME_DIFF_MIN = first( COLL_TIME_DIFF_MIN ),
      AGE_PROC           = first( AGE_PROC           ),
      ENC_KEY            = first( ENC_KEY )  
    ) %>%
    dplyr::ungroup()

  cat(sprintf('Number of non-duplicated first PROC_NAME rows: %d\n',
              nrow(unique.x.df)))
    
  # Similarly, ensure that each second PROC_NAME order is being used just once
  # (i.e., that ORDER_PROC_KEY.y is not duplicated)
  non.dup.df <-
    unique.x.df %>%
    dplyr::arrange(ORDER_PROC_KEY.y, COLL_TIME_DIFF_MIN) %>%
    dplyr::group_by(ORDER_PROC_KEY.y) %>%
    dplyr::summarize(
      ORDER_PROC_KEY.x   = first( ORDER_PROC_KEY.x   ),
      DEPT               = first( DEPT               ),
      COLLECTED_DT.x     = first( COLLECTED_DT.x     ),
      RESULT_DT.x        = first( RESULT_DT.x        ),
      NUM_VAL.x          = first( NUM_VAL.x          ),
      COLLECTED_DT.y     = first( COLLECTED_DT.y     ),
      RESULT_DT.y        = first( RESULT_DT.y        ),
      NUM_VAL.y          = first( NUM_VAL.y          ),
      COLL_TIME_DIFF_MIN = first( COLL_TIME_DIFF_MIN ),
      AGE_PROC           = first( AGE_PROC           ),      
      ENC_KEY            = first( ENC_KEY )
    ) %>%
    dplyr::ungroup()
  
  cat(sprintf('Number of non-duplicated second PROC_NAME rows: %d\n',
              nrow(non.dup.df)))
    
  # Do we limit by one per patient?
  if (!multi.per.pt) {
    per.pt.df <-
      non.dup.df %>%
      # Sort by PAT_KEY and the first COLLECTED DT
      dplyr::arrange(PAT_KEY, COLLECTED_DT.x) %>%
      # Group by PAT_KEY and add a "LINE" number 
      dplyr::group_by(PAT_KEY) %>%
      dplyr::mutate(
        PAT_LINE = row_number()
      ) %>%
      # Ungroup
      dplyr::ungroup() %>%
      # Filter for lines == 1 only
      dplyr::filter(PAT_LINE == 1) %>%
      dplyr::select(-PAT_LINE)
  } else {
    per.pt.df <- non.dup.df
  }
  
  cat(sprintf('Number of paired, simultaneous values: %d\n',
              nrow(per.pt.df)))
  
  cat(sprintf('Number of duplicated ORDER_PROC_KEY.x values: %d\n',
              sum(duplicated(per.pt.df$ORDER_PROC_KEY.x))))
  
  return(per.pt.df)
}
```

First we create the CBC - BG dataset using the primary cutoff value, and include all pairs per patient.

```{r}
cbc.bg <- createPairedDataset(
  labs.df = labs.df, 
  cohort.df = cohort.df,
  PN = c('CBC', 'BG'), 
  CN = 'Hgb',
  time.diff = primary.cutoff,
  multi.per.pt = T
)
```

## Analyze

### Error Grid

We begin the assessment of clinical accuracy by creating the Error Grid.

```{r}
#'
#' @title Calculate Error Grid
#' 
#' @description Calculates points within each area of Error Grid and plots
#' 
#' @param df The paired samples data frame, calculated above
#' @param to.plot If TRUE [Default], displays the Error Grid plot
#' @param to.return If TRUE [Default], returns the plot
#'
calculateErrorGrid <- function (df, to.plot = T, to.return = T) {
  
  #'
  #' Sub-function to define the underlying grid pts
  #' 
  makeBaseGrid <- function () {
    
    # Define the points which comprise the Error Grid
    A <- data.frame(
      X = c(0, 6, 6, 10, 25, 25, 9, 9, 5.4, 0),
      Y = c(0, 0, 5.4, 9, 9, 25, 25, 10, 6, 6))
    
    B <- data.frame(
      X = c(0, 25, 25, 0),
      Y = c(0, 0, 25, 25))
    
    C <- data.frame(
      X = c(0, 6, 6, 0),
      Y = c(10, 10, 25, 25))
    
    D <- data.frame(
      X = c(10, 10, 25, 25),
      Y = c(0, 6, 6, 0))
    
    # Generate grid
    p <- 
      ggplot() + 
      geom_abline(mapping = NULL, data = NULL,
                  slope = 1, intercept = 0, na.rm = FALSE, 
                  show.legend = NA, size = 1) + 
      geom_polygon(aes(x = X, y = Y), size = 1.5, color = 'green', 
                   fill = 'green', alpha = 0.2, data = A) + 
      geom_polygon(aes(x = X, y = Y), size = 1.5, color = 'yellow', 
                   fill = 'yellow', alpha = 0.1, data = B) +
      geom_polygon(aes(x = X, y = Y), size = 1.5, color = 'red', 
                   fill = 'red', alpha = 0.2, data = C) +
      geom_polygon(aes(x = X, y = Y), size = 1.5, color = 'red', 
                   fill = 'red', alpha = 0.2, data = D) + 
      coord_cartesian(ylim = c(4, 20), xlim = c(4, 20)) + 
      scale_fill_distiller(palette = 4, direction = 1) 

    return(list(
      A = A, B = B, C = C, D = D, p = p
    ))
  } # End of sub-function
  
  g <- makeBaseGrid()
  
  Queries <- as.matrix(df %>% dplyr::select(NUM_VAL.x, NUM_VAL.y))

  A.res <- ptinpoly::pip2d(Vertices = as.matrix(g$A), Queries = Queries)
  B.res <- ptinpoly::pip2d(Vertices = as.matrix(g$B), Queries = Queries)
  C.res <- ptinpoly::pip2d(Vertices = as.matrix(g$C), Queries = Queries)
  D.res <- ptinpoly::pip2d(Vertices = as.matrix(g$D), Queries = Queries)
  
  # First display raw "Box" output
  cat(sprintf(paste0(
    'Counts by Box:\n',
    '\tBox A: %d (%0.1f %%)\n',
    '\tBox B: %d (%0.1f %%)\n',
    '\tBox C: %d (%0.1f %%)\n',
    '\tBox D: %d (%0.1f %%)\n'),
    sum(A.res >= 0), sum(A.res >= 0) / length(A.res) * 100.,
    sum(B.res >= 0), sum(B.res >= 0) / length(A.res) * 100.,
    sum(C.res >= 0), sum(C.res >= 0) / length(A.res) * 100.,
    sum(D.res >= 0), sum(D.res >= 0) / length(A.res) * 100.))
  
  # Now display by Green, Yellow, Red
  #   Note that Green = A, Yellow = B - A - C - D, Red = C + D
  cat(sprintf(paste0(
    'Counts by Area:\n',
    '\tGreen Area: %d (%0.2f %%)\n',
    '\tYellow Area: %d (%0.2f %%)\n',
    '\tRed Area: %d (%0.2f %%)\n'),
    sum(A.res >= 0), sum(A.res >= 0) / length(A.res) * 100.,
    sum(B.res >= 0) - sum(A.res >= 0) - sum(C.res >= 0) - sum(D.res >= 0),
    ( sum(B.res >= 0) - sum(A.res >= 0) - sum(C.res >= 0) - sum(D.res >= 0) ) /
      length(A.res) * 100.,
    sum(C.res >= 0) + sum(D.res >= 0),
    (sum(C.res >= 0) + sum(D.res >= 0) ) / length(A.res) * 100.
  ))
  
  # Plot and return (pending parameters)
  if (to.plot & to.return) {
    Error_Grid <- 
      g$p +
      geom_jitter(aes(x = NUM_VAL.x, y = NUM_VAL.y), data = df, 
                  width = 0.3, height = 0.3, size = 0.3) + 
      xlab('Reference Lab Value') + 
      ylab('Measured Lab Value') + 
      theme_bw() + 
      theme(panel.grid.minor = element_blank()) + 
      theme(panel.background = element_rect(fill = "transparent", colour = NA),
            plot.background = element_rect(fill = "transparent", colour = NA))
  
  } else if (to.plot) {
    print(Error_Grid)
    
    return()
    
  } else if (to.return) {
    return(Error_Grid)
    
  } else {
    return()
    
  }
}
```

Now we use this function to calculate Error Grid counts and display plot:

```{r, fig.width = 5, fig.height = 5}
error.grid.cbc.bg <- calculateErrorGrid(
  df = cbc.bg,
  to.plot = T,
  to.return = T
)
```

### Regression Analysis

To complete regression analysis, we must first gather the other variables from the original `labs.df` data frame. Then we set a "well-matched" threshold and run the regression model.

To do this properly on the UR system, which utilizes both "blood gas" and "blood gas panel" orders for different components of the blood gas, we need to join on `ENC_KEY` and by `COLLECTED_DT` because `ORDER_PROC_KEY` does *NOT* generate the full match.

```{r}
#'
#' @title Gather Covariates
#' 
#' @description Creates pivoted data frame of covariate labs for each pair
#' 
#' @details Requires that the column `ORDER_PROC_KEY.x` is unique in the 
#'     paired dataframe. This will be true when the data frame is created
#'     using the above function `createPairedDataset()` which catches duplicates.
#'     
#'     To find covariates, instead of matching on the `ORDER_PROC_KEY.x` which
#'     is correct for some of the components, matches on the PN[2] collected time
#'     (`COLLECTED_DT.y`) and the encounter key. These are checked to be unique
#'     in the input joined data frame as well.
#' 
#' @param paired.df The paired dataframe containing unique values at the 
#'     `ORDER_PROC_KEY.x` column, which is the column for the order key for
#'     PN[1] (typically the CBC). Also must contain (at least) the columns
#'     `COLLECTED_DT.y` and `ENC_KEY`
#' @param labs.df The full labs data frame
#' @param covars A list of covariate names into the `COMP_NAME` column
#' 
#' @returns A pivoted data frame of covariates
#'
gatherCovariates <- function (paired.df, labs.df, 
                              covars = c('pH', 'Bicarb', 'iCal', 'Gluc', 'Lactate')) {
  
  # Ensure that these are all unique
  if (length(unique(paired.df$ORDER_PROC_KEY.x)) != 
      length(paired.df$ORDER_PROC_KEY.x))
    stop('PN[1] Order Proc Keys should be unique')
  
  # Ensure that collected time of PN[2] and ENC_KEY are distinct
  if (nrow(paired.df) != 
      nrow(paired.df %>% 
           dplyr::select(COLLECTED_DT.y, ENC_KEY) %>% 
           dplyr::distinct()))
    stop('COLLECTED_DT.y and ENC_KEY tuple are not distinct in paired data frame')
  
  # Filter to remove any cancelled labs or NaNs
  filtered.df <-
    labs.df %>%
    dplyr::filter(!is.na(NUM_VAL) & NUM_VAL != 9999999.)
  
  # Initialize the result data frame using the unique `ORDER_PROC_KEY.x` values
  result.df <- data.frame(
    ORDER_PROC_KEY.x = paired.df$ORDER_PROC_KEY.x
  )
  
  cat(sprintf('NUmber of unique PN[1] order procedure keys: %d\n',
              nrow(result.df)))

  # Join the filtered data frame to full lab results and find matches
  # on the ENC_KEY and COLLECTED_DT.y
  joined.df <-
    dplyr::inner_join(
      x = paired.df %>%
        dplyr::select(ORDER_PROC_KEY.x, ENC_KEY, COLLECTED_DT.y),
      y = filtered.df,
      by = c('ENC_KEY', 'COLLECTED_DT.y' = 'COLLECTED_DT')
    )
  
  # Join each component to the results data frame
  for (CN in covars) {
    
    result.df <-
      joined.df %>%
      dplyr::filter(COMP_NAME == CN) %>%
      dplyr::select(ORDER_PROC_KEY.x, NUM_VAL, RESULT_DT) %>%
      dplyr::arrange(ORDER_PROC_KEY.x, RESULT_DT) %>%
      dplyr::group_by(ORDER_PROC_KEY.x) %>%
      dplyr::summarize(
        LAST_ADD = first(NUM_VAL)
      ) %>%
      dplyr::ungroup() %>%
      dplyr::right_join(
        y = result.df,
        by = c('ORDER_PROC_KEY.x')
      )
    
    names(result.df)[which(names(result.df) == 'LAST_ADD')] <- CN
  }
  
  return(result.df)
}
```

Here we utilize the `gatherCovariates()` function to extract covariates for our paired dataset:

```{r}
# Gather covariates for this paired set
covars.df <- gatherCovariates(cbc.bg, labs.df)
```

Now we display stats on the covariates, to ensure we have appropriately filled the table and show the distributions. TO do this, we write a function which iterates across the columns:

```{r}
#'
#' @title Display Covariate Stats
#' 
#' @description Displays statistics on covariates in the data frame
#' 
#' @param covars.df The Covariates data frame from `gatherCovariates()` function
#' 
displayCovariateStats <- function (covars.df) {

  # Display stats on the covariates, including checking for NULL values and
  # displaying distributions
  for (index in 2 : ncol(covars.df)) {
    
    this.vec <- covars.df[,index]
    
    print(summary(this.vec))
    
    cat(sprintf('Count (and %%) of NAs in %s column: %d (%0.2f %%)\n',
                names(covars.df)[index],
                sum(is.na(this.vec)),
                sum(is.na(this.vec)) / nrow(covars.df) * 100.))
    
    bounds <- quantile(this.vec, probs = c(0.01, 0.99), na.rm = T)
    
    this.df <-
      covars.df %>%
      dplyr::select(all_of(index))
    
    names(this.df) <- c('val')
    
    this.filt.df <-
      this.df %>%
      dplyr::filter(val >= bounds[1] & val <= bounds[2])
    
    hist.bins <- min(
      length(unique(this.filt.df$val)),
      40)
    
    p <-
      this.filt.df %>%
      ggplot() +
      geom_histogram(aes(x = val), bins = hist.bins) +
      xlab(paste0(names(covars.df)[index], ' values (1st - 99th percentile)')) +
      ylab('Count') +
      theme_bw()
    
    print(p)
  }
  
}

```

```{r}
displayCovariateStats(covars.df)
```

Now we join the covariates to the paired dataframe and add the age, filter and impute. Then we run the logistic regression and display the results:

```{r}
#'
#' @title Filter Impute Regress
#' 
#' @description ...
#'
filterImputeRegress <- function (paired.df, covars.df, impute.fx = median, ci = T) {
  
  # First we join the paired data frame (as the basis) with the covariates,
  # by the unique ORDER PROC key of PN[1] (should be CBC)
  joined.df <-
    dplyr::left_join(
      x = paired.df, 
      y = covars.df,
      by = c('ORDER_PROC_KEY.x')
    ) %>%
    dplyr:: select(NUM_VAL.x, NUM_VAL.y, AGE_PROC, pH, Bicarb, iCal, Gluc, Lactate, DEPT)
  
  # Impute NA values based on the impute function
  impute.df <-
    joined.df %>%
    dplyr::mutate(
      pH =      ifelse( is.na(pH),      impute.fx(joined.df$pH, na.rm = T),      pH),
      Gluc =    ifelse( is.na(Gluc),    impute.fx(joined.df$Gluc, na.rm = T),    Gluc),
      iCal =    ifelse( is.na(iCal),    impute.fx(joined.df$iCal, na.rm = T),    iCal),
      Lactate = ifelse( is.na(Lactate), impute.fx(joined.df$Lactate, na.rm = T), Lactate),
      Bicarb =  ifelse( is.na(Bicarb),  impute.fx(joined.df$Bicarb, na.rm = T),  Bicarb)
    )
  
  # Threshold to create "WELL_MATCHED" column
  thresh.df <-
    impute.df %>%
    dplyr::mutate(
      MEAN_HGB = (NUM_VAL.x + NUM_VAL.y) / 2.,
      DIFF_HGB = abs(NUM_VAL.x - NUM_VAL.y),
      WELL_MATCHED = case_when(
        MEAN_HGB > 6 & MEAN_HGB < 9 ~ if_else(DIFF_HGB < 1, TRUE, FALSE), 
        TRUE ~ if_else(DIFF_HGB < 1.5, TRUE, FALSE)
      )
    ) %>%
    dplyr::select(-MEAN_HGB, -DIFF_HGB, -NUM_VAL.x)
  
  cat(sprintf('Number (%%) of `WELL MATCHED`: %d (%0.2f %%)\n',
              sum(thresh.df$WELL_MATCHED),
              sum(thresh.df$WELL_MATCHED) / nrow(thresh.df) * 100.))
  
  reg.model <- glm(
    WELL_MATCHED ~ NUM_VAL.y +
      pH + 
      Gluc +
      Bicarb +
      iCal +
      Lactate +
      AGE_PROC + 
      DEPT,
    family = 'binomial',
    data = thresh.df %>%
      dplyr::mutate(pH = pH * 10.)
  )
  
  print(summary(reg.model))
  
  print(exp(reg.model$coefficients))
  
  if (ci) {
    
    ci.reg <- confint(reg.model)
    
    print(exp(ci.reg))
    
    return(list(
      reg.model,
      ci.reg
    ))
    
  } else {
    return(list(
      reg.model
    ))
  }
}
```

Now run the joining, imputing, and regression:

```{r}
regress.res <-
  filterImputeRegress(
    paired.df = cbc.bg,
    covars.df = covars.df,
    impute.fx = median,
    ci = F
  )
```
