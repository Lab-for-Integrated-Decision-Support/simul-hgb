---
title: "03_Clinical_Accuracy"
author: "Adam Dziorny"
date: '2022-10-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The purpose of this markdown is to report clinical accuracy by displaying the Error Grid analysis, computing logistic regression on "well-matched" pairs, calculating Cohen's Kappa, and our "transfusion test" results. 

## Initialize

First we load the necessary packages:

```{r, warning=FALSE}
suppressPackageStartupMessages({
  
  # Data frame manipulation
  require(dplyr)
  
  # Graphics and output
  require(ggplot2)
  #require(cowplot)
  
  # Tables
  require(knitr)
  require(kableExtra)
  
  # Error grid point allocation
  require(ptinpoly)

})
```

Ensure the environmental variables are specified:

```{r}
if (Sys.getenv('PICU_LAB_DATA_PATH') == '' |
    Sys.getenv('PICU_LAB_IMG_PATH') == '' |
    Sys.getenv('PICU_LAB_IN_FILE') == '' |
    Sys.getenv('PICU_LAB_SITE_NAME') == '')
  stop('Missing necessary environmental variables - see README.md')
```

Specify the run date:

```{r}
run.date <- '2022-10-05'
```

## Data Input

Load data from the `DATA_PATH` with the associated `IN_FILE`, adding a file separator between them. This should result in loading two data frames: `cohort.df` and `labs.df`.

```{r}
load(
  file = file.path(
    Sys.getenv('PICU_LAB_DATA_PATH'),
    Sys.getenv('PICU_LAB_IN_FILE')
  )
)
```

## Set Parameters

We will utilize several sensitivity analyses in this markdown - these should be identical to the sensitivity indicators in the prior markdown `02_Analytic_Accuracy.Rmd`. No changes should be made to these parameters - only additions for more sensitivity parameters.

```{r}
# The primary cutoff value between collection times (in minutes) to 
# determine "simultaneous"
primary.cutoff <- 15. 

# Sensitivity analysis list
sens.cutoffs <- c(1., 30., 90.)

# Hgb cutoffs
primary.hgb.cutoff <- 7.0

sens.hgb.cutoffs <- c(5., 9.)
```

## Join

The below join function is a copy of the function used in `02_Analytic_Accuracy.Rmd`. No changes should be made to this version - make changes to the prior version, re-test within that script, and then copy here. After development, this will be moved to a package.

```{r}
#'
#' @title Create Paired Dataset
#' 
#' @description Creates a dataset of paired simultaneous lab values
#'
#' @param labs.df The labs data frame
#' @param cohort.df The cohort data frame, needed for PAT_KEY and DEPT
#' @param PN A two-element list of PROC_NAMEs to join
#' @param time.diff The max time difference (min) between collected times
#' @param CN The COMP_NAME to join [Default: 'Hgb']
#' @param multi.per.pt If FALSE, limit to first result per patient, otherwise 
#'     if TRUE [Default], allow all
#'     
#' @returns The resulting joined data frame
#'
createPairedDataset <- function (labs.df, cohort.df, PN, time.diff, 
                           CN = 'Hgb', multi.per.pt = T) {
  
  # First we filter to remove the non-numeric rows
  filter.df <- 
    labs.df %>%
    dplyr::filter(!is.na(NUM_VAL) & NUM_VAL != 9999999.) %>%
    dplyr::filter(COMP_NAME == CN)
  
  cat(sprintf('Number of component numeric rows in input data frame: %d\n',
              nrow(filter.df)))
  
  # Join to get PAT_KEY and DEPT, used in subsequent filtering
  keyed.df <- 
    dplyr::left_join(
      x = filter.df,
      y = cohort.df %>% 
        dplyr::select(ENC_KEY, PAT_KEY, DEPT),
      by = c('ENC_KEY')
    )
  
  # Now we filter by PN and join to create full data frame
  joined.df <-
    dplyr::inner_join(
      x = keyed.df %>%
        dplyr::filter(PROC_NAME == PN[1]) %>%
        dplyr::select(ENC_KEY, PAT_KEY, ORDER_PROC_KEY, 
                      DEPT, COLLECTED_DT, RESULT_DT, NUM_VAL, AGE_PROC),
      y = keyed.df %>%
        dplyr::filter(PROC_NAME == PN[2]) %>%
        dplyr::select(ENC_KEY, PAT_KEY, ORDER_PROC_KEY,
                      DEPT, COLLECTED_DT, RESULT_DT, NUM_VAL),
      by = c('ENC_KEY', 'PAT_KEY', 'DEPT'),
      suffix = c('.x', '.y')
    ) 
  
  # Join using base R, by column number
  #   [[5]] is PN[1] COLLECTED_DT
  #   [[10]] is PN[2] COLLECTED_DT
  joined.df$COLL_TIME_DIFF_MIN <-
    as.numeric(joined.df[[5]] - joined.df[[10]], units = 'mins')
 
  # Apply the cutoff time
  cutoff.df <- 
    joined.df %>%
    dplyr::filter(abs(COLL_TIME_DIFF_MIN) < time.diff)
  
  cat(sprintf('Number of paired, simultaneous values meeting cutoff: %d\n',
              nrow(cutoff.df)))
    
  # Ensure that each first PROC_NAME order is only used once - meaning that  
  # each ORDER_PROC_KEY.x should be unique
  unique.x.df <- 
    cutoff.df%>%
    dplyr::arrange(ORDER_PROC_KEY.x, COLL_TIME_DIFF_MIN) %>%
    dplyr::group_by(ORDER_PROC_KEY.x) %>%
    dplyr::summarize(
      ORDER_PROC_KEY.y   = first( ORDER_PROC_KEY.y   ),
      DEPT               = first( DEPT               ),
      COLLECTED_DT.x     = first( COLLECTED_DT.x     ),
      RESULT_DT.x        = first( RESULT_DT.x        ),
      NUM_VAL.x          = first( NUM_VAL.x          ),
      COLLECTED_DT.y     = first( COLLECTED_DT.y     ),
      RESULT_DT.y        = first( RESULT_DT.y        ),
      NUM_VAL.y          = first( NUM_VAL.y          ),
      COLL_TIME_DIFF_MIN = first( COLL_TIME_DIFF_MIN ),
      AGE_PROC           = first( AGE_PROC           ),
      ENC_KEY            = first( ENC_KEY            ),
      PAT_KEY            = first( PAT_KEY            )
    ) %>%
    dplyr::ungroup()

  cat(sprintf('Number of non-duplicated first PROC_NAME rows: %d\n',
              nrow(unique.x.df)))
    
  # Similarly, ensure that each second PROC_NAME order is being used just once
  # (i.e., that ORDER_PROC_KEY.y is not duplicated)
  non.dup.df <-
    unique.x.df %>%
    dplyr::arrange(ORDER_PROC_KEY.y, COLL_TIME_DIFF_MIN) %>%
    dplyr::group_by(ORDER_PROC_KEY.y) %>%
    dplyr::summarize(
      ORDER_PROC_KEY.x   = first( ORDER_PROC_KEY.x   ),
      DEPT               = first( DEPT               ),
      COLLECTED_DT.x     = first( COLLECTED_DT.x     ),
      RESULT_DT.x        = first( RESULT_DT.x        ),
      NUM_VAL.x          = first( NUM_VAL.x          ),
      COLLECTED_DT.y     = first( COLLECTED_DT.y     ),
      RESULT_DT.y        = first( RESULT_DT.y        ),
      NUM_VAL.y          = first( NUM_VAL.y          ),
      COLL_TIME_DIFF_MIN = first( COLL_TIME_DIFF_MIN ),
      AGE_PROC           = first( AGE_PROC           ),      
      ENC_KEY            = first( ENC_KEY            ),
      PAT_KEY            = first( PAT_KEY            )
    ) %>%
    dplyr::ungroup()
  
  cat(sprintf('Number of non-duplicated second PROC_NAME rows: %d\n',
              nrow(non.dup.df)))
    
  # Do we limit by one per patient?
  if (!multi.per.pt) {
    per.pt.df <-
      non.dup.df %>%
      # Sort by PAT_KEY and the first COLLECTED DT
      dplyr::arrange(PAT_KEY, COLLECTED_DT.x) %>%
      # Group by PAT_KEY and add a "LINE" number 
      dplyr::group_by(PAT_KEY) %>%
      dplyr::mutate(
        PAT_LINE = row_number()
      ) %>%
      # Ungroup
      dplyr::ungroup() %>%
      # Filter for lines == 1 only
      dplyr::filter(PAT_LINE == 1) %>%
      dplyr::select(-PAT_LINE)
  } else {
    per.pt.df <- non.dup.df
  }
  
  cat(sprintf('Number of paired, simultaneous values: %d\n',
              nrow(per.pt.df)))
  
  cat(sprintf('Number of duplicated ORDER_PROC_KEY.x values: %d\n',
              sum(duplicated(per.pt.df$ORDER_PROC_KEY.x))))
  
  return(per.pt.df)
}
```

First we create the CBC - BG dataset using the primary cutoff value, and include all pairs per patient.

```{r}
cbc.bg <- createPairedDataset(
  labs.df = labs.df, 
  cohort.df = cohort.df,
  PN = c('CBC', 'BG'), 
  CN = 'Hgb',
  time.diff = primary.cutoff,
  multi.per.pt = T
)
```

## Analyze

In this section we complete the clinical accuracy assessments.

### Error Grid

We begin the assessment of clinical accuracy by creating the Error Grid.

```{r}
#'
#' @title Calculate Error Grid
#' 
#' @description Calculates points within each area of Error Grid and plots
#' 
#' @param df The paired samples data frame, calculated above
#' @param to.plot If TRUE [Default], displays the Error Grid plot
#' @param to.return If TRUE [Default], returns the plot
#'
calculateErrorGrid <- function (df, to.plot = T, to.return = T) {
  
  #'
  #' Sub-function to define the underlying grid pts
  #' 
  makeBaseGrid <- function () {
    
    # Define the points which comprise the Error Grid
    A <- data.frame(
      X = c(0, 6, 6, 10, 25, 25, 9, 9, 5.4, 0),
      Y = c(0, 0, 5.4, 9, 9, 25, 25, 10, 6, 6))
    
    B <- data.frame(
      X = c(0, 25, 25, 0),
      Y = c(0, 0, 25, 25))
    
    C <- data.frame(
      X = c(0, 6, 6, 0),
      Y = c(10, 10, 25, 25))
    
    D <- data.frame(
      X = c(10, 10, 25, 25),
      Y = c(0, 6, 6, 0))
    
    # Generate grid
    p <- 
      ggplot() + 
      geom_abline(mapping = NULL, data = NULL,
                  slope = 1, intercept = 0, na.rm = FALSE, 
                  show.legend = NA, size = 1) + 
      geom_polygon(aes(x = X, y = Y), size = 1.5, color = 'green', 
                   fill = 'green', alpha = 0.2, data = A) + 
      geom_polygon(aes(x = X, y = Y), size = 1.5, color = 'yellow', 
                   fill = 'yellow', alpha = 0.1, data = B) +
      geom_polygon(aes(x = X, y = Y), size = 1.5, color = 'red', 
                   fill = 'red', alpha = 0.2, data = C) +
      geom_polygon(aes(x = X, y = Y), size = 1.5, color = 'red', 
                   fill = 'red', alpha = 0.2, data = D) + 
      coord_cartesian(ylim = c(4, 20), xlim = c(4, 20)) + 
      scale_fill_distiller(palette = 4, direction = 1) 

    return(list(
      A = A, B = B, C = C, D = D, p = p
    ))
  } # End of sub-function
  
  g <- makeBaseGrid()
  
  Queries <- as.matrix(df %>% dplyr::select(NUM_VAL.x, NUM_VAL.y))

  A.res <- ptinpoly::pip2d(Vertices = as.matrix(g$A), Queries = Queries)
  B.res <- ptinpoly::pip2d(Vertices = as.matrix(g$B), Queries = Queries)
  C.res <- ptinpoly::pip2d(Vertices = as.matrix(g$C), Queries = Queries)
  D.res <- ptinpoly::pip2d(Vertices = as.matrix(g$D), Queries = Queries)
  
  # First display raw "Box" output
  cat(sprintf(paste0(
    'Counts by Box:\n',
    '\tBox A: %d (%0.1f %%)\n',
    '\tBox B: %d (%0.1f %%)\n',
    '\tBox C: %d (%0.1f %%)\n',
    '\tBox D: %d (%0.1f %%)\n'),
    sum(A.res >= 0), sum(A.res >= 0) / length(A.res) * 100.,
    sum(B.res >= 0), sum(B.res >= 0) / length(A.res) * 100.,
    sum(C.res >= 0), sum(C.res >= 0) / length(A.res) * 100.,
    sum(D.res >= 0), sum(D.res >= 0) / length(A.res) * 100.))
  
  # Now display by Green, Yellow, Red
  #   Note that Green = A, Yellow = B - A - C - D, Red = C + D
  cat(sprintf(paste0(
    'Counts by Area:\n',
    '\tGreen Area: %d (%0.2f %%)\n',
    '\tYellow Area: %d (%0.2f %%)\n',
    '\tRed Area: %d (%0.2f %%)\n'),
    sum(A.res >= 0), sum(A.res >= 0) / length(A.res) * 100.,
    sum(B.res >= 0) - sum(A.res >= 0) - sum(C.res >= 0) - sum(D.res >= 0),
    ( sum(B.res >= 0) - sum(A.res >= 0) - sum(C.res >= 0) - sum(D.res >= 0) ) /
      length(A.res) * 100.,
    sum(C.res >= 0) + sum(D.res >= 0),
    (sum(C.res >= 0) + sum(D.res >= 0) ) / length(A.res) * 100.
  ))
  
  # Plot and return (pending parameters)
  if (to.plot & to.return) {
    Error_Grid <- 
      g$p +
      geom_jitter(aes(x = NUM_VAL.x, y = NUM_VAL.y), data = df, 
                  width = 0.3, height = 0.3, size = 0.3) + 
      xlab('Reference Lab Value') + 
      ylab('Measured Lab Value') + 
      theme_bw() + 
      theme(panel.grid.minor = element_blank()) + 
      theme(panel.background = element_rect(fill = "transparent", colour = NA),
            plot.background = element_rect(fill = "transparent", colour = NA))
  
  } 
  
  if (to.plot) {
    print(Error_Grid)
  } 
  
  if (to.return) {
    return(Error_Grid)
    
  } else {
    return()
    
  }
}
```

Now we use this function to calculate Error Grid counts and display plot:

```{r, fig.width = 5, fig.height = 5}
error.grid.cbc.bg <- calculateErrorGrid(
  df = cbc.bg,
  to.plot = T,
  to.return = T
)
```

### Regression Analysis

To complete regression analysis, we must first gather the other variables from the original `labs.df` data frame. Then we set a "well-matched" threshold and run the regression model.

To do this properly on the UR system, which utilizes both "blood gas" and "blood gas panel" orders for different components of the blood gas, we need to join on `ENC_KEY` and by `COLLECTED_DT` because `ORDER_PROC_KEY` does *NOT* generate the full match.

```{r}
#'
#' @title Gather Covariates
#' 
#' @description Creates pivoted data frame of covariate labs for each pair
#' 
#' @details Requires that the column `ORDER_PROC_KEY.x` is unique in the 
#'     paired dataframe. This will be true when the data frame is created
#'     using the above function `createPairedDataset()` which catches duplicates.
#'     
#'     To find covariates, instead of matching on the `ORDER_PROC_KEY.x` which
#'     is correct for some of the components, matches on the PN[2] collected time
#'     (`COLLECTED_DT.y`) and the encounter key. These are checked to be unique
#'     in the input joined data frame as well.
#' 
#' @param paired.df The paired dataframe containing unique values at the 
#'     `ORDER_PROC_KEY.x` column, which is the column for the order key for
#'     PN[1] (typically the CBC). Also must contain (at least) the columns
#'     `COLLECTED_DT.y` and `ENC_KEY`
#' @param labs.df The full labs data frame
#' @param covars A list of covariate names into the `COMP_NAME` column
#' 
#' @returns A pivoted data frame of covariates
#'
gatherCovariates <- function (paired.df, labs.df, 
                              covars = c('pH', 'Bicarb', 'iCal', 'Gluc', 'Lactate')) {
  
  # Ensure that these are all unique
  if (length(unique(paired.df$ORDER_PROC_KEY.x)) != 
      length(paired.df$ORDER_PROC_KEY.x))
    stop('PN[1] Order Proc Keys should be unique')
  
  # Ensure that collected time of PN[2] and ENC_KEY are distinct
  if (nrow(paired.df) != 
      nrow(paired.df %>% 
           dplyr::select(COLLECTED_DT.y, ENC_KEY) %>% 
           dplyr::distinct()))
    stop('COLLECTED_DT.y and ENC_KEY tuple are not distinct in paired data frame')
  
  # Filter to remove any cancelled labs or NaNs
  filtered.df <-
    labs.df %>%
    dplyr::filter(!is.na(NUM_VAL) & NUM_VAL != 9999999.)
  
  # Initialize the result data frame using the unique `ORDER_PROC_KEY.x` values
  result.df <- data.frame(
    ORDER_PROC_KEY.x = paired.df$ORDER_PROC_KEY.x
  )
  
  cat(sprintf('NUmber of unique PN[1] order procedure keys: %d\n',
              nrow(result.df)))

  # Join the filtered data frame to full lab results and find matches
  # on the ENC_KEY and COLLECTED_DT.y
  joined.df <-
    dplyr::inner_join(
      x = paired.df %>%
        dplyr::select(ORDER_PROC_KEY.x, ENC_KEY, COLLECTED_DT.y),
      y = filtered.df,
      by = c('ENC_KEY', 'COLLECTED_DT.y' = 'COLLECTED_DT')
    )
  
  # Join each component to the results data frame
  for (CN in covars) {
    
    result.df <-
      joined.df %>%
      dplyr::filter(COMP_NAME == CN) %>%
      dplyr::select(ORDER_PROC_KEY.x, NUM_VAL, RESULT_DT) %>%
      dplyr::arrange(ORDER_PROC_KEY.x, RESULT_DT) %>%
      dplyr::group_by(ORDER_PROC_KEY.x) %>%
      dplyr::summarize(
        LAST_ADD = first(NUM_VAL)
      ) %>%
      dplyr::ungroup() %>%
      dplyr::right_join(
        y = result.df,
        by = c('ORDER_PROC_KEY.x')
      )
    
    names(result.df)[which(names(result.df) == 'LAST_ADD')] <- CN
  }
  
  return(result.df)
}
```

Here we utilize the `gatherCovariates()` function to extract covariates for our paired dataset:

```{r}
# Gather covariates for this paired set
covars.df <- gatherCovariates(cbc.bg, labs.df)
```

Now we display stats on the covariates, to ensure we have appropriately filled the table and show the distributions. TO do this, we write a function which iterates across the columns:

```{r}
#'
#' @title Display Covariate Stats
#' 
#' @description Displays statistics on covariates in the data frame
#' 
#' @param covars.df The Covariates data frame from `gatherCovariates()` function
#' 
displayCovariateStats <- function (covars.df) {

  # Display stats on the covariates, including checking for NULL values and
  # displaying distributions
  for (index in 2 : ncol(covars.df)) {
    
    this.vec <- covars.df[,index]
    
    print(summary(this.vec))
    
    cat(sprintf('Count (and %%) of NAs in %s column: %d (%0.2f %%)\n',
                names(covars.df)[index],
                sum(is.na(this.vec)),
                sum(is.na(this.vec)) / nrow(covars.df) * 100.))
    
    bounds <- quantile(this.vec, probs = c(0.01, 0.99), na.rm = T)
    
    this.df <-
      covars.df %>%
      dplyr::select(all_of(index))
    
    names(this.df) <- c('val')
    
    this.filt.df <-
      this.df %>%
      dplyr::filter(val >= bounds[1] & val <= bounds[2])
    
    hist.bins <- min(
      length(unique(this.filt.df$val)),
      40)
    
    p <-
      this.filt.df %>%
      ggplot() +
      geom_histogram(aes(x = val), bins = hist.bins) +
      xlab(paste0(names(covars.df)[index], ' values (1st - 99th percentile)')) +
      ylab('Count') +
      theme_bw()
    
    print(p)
  }
  
}

```

```{r}
displayCovariateStats(covars.df)
```

Now we join the covariates to the paired dataframe and add the age, filter and impute. Then we run the logistic regression and display the results:

```{r}
#'
#' @title Join Impute Regress
#' 
#' @description Join covars and pairs, impute NA values, regress, and report results
#' 
#' @param paired.df A dataframe of paired PN[1] and PN[2] values, created from 
#'     the function `createPairedDataset()`
#' @param covars.df A dataframe of covariates, created from the 
#'     function `gatherCovariates()`
#' @param thresh.list A list of three-element vectors, where each three-element
#'     vector is of the format c(min, max, thresh). Such that for the list of 
#'     pairs, if the mean Hgb (between PN[1] and PN[2]) falls between `min` and 
#'     `max` and the difference is less than the threshold `thresh`, then the 
#'     pair is considered `WELL_MATCHED` for the purposes of our logistic 
#'     regression.
#' @param impute.fx The function for imputing NA values [Default: `median`]
#' @param ci If TRUE, compute the confidence intervals on the regression results 
#'     [Default: TRUE]
#'     
#' @returns A list of the regression results as well as CIs (if computed)
#'
joinImputeRegress <- function (paired.df, covars.df, thresh.list, 
                                 impute.fx = median, ci = T) {
  
  # First we join the paired data frame (as the basis) with the covariates,
  # by the unique ORDER PROC key of PN[1] (should be CBC)
  joined.df <-
    dplyr::left_join(
      x = paired.df, 
      y = covars.df,
      by = c('ORDER_PROC_KEY.x')
    ) %>%
    dplyr:: select(NUM_VAL.x, NUM_VAL.y, AGE_PROC, pH, Bicarb, iCal, Gluc, Lactate, DEPT)
  
  # Impute NA values based on the impute function
  impute.df <-
    joined.df %>%
    dplyr::mutate(
      pH =      ifelse( is.na(pH),      impute.fx(joined.df$pH, na.rm = T),      pH),
      Gluc =    ifelse( is.na(Gluc),    impute.fx(joined.df$Gluc, na.rm = T),    Gluc),
      iCal =    ifelse( is.na(iCal),    impute.fx(joined.df$iCal, na.rm = T),    iCal),
      Lactate = ifelse( is.na(Lactate), impute.fx(joined.df$Lactate, na.rm = T), Lactate),
      Bicarb =  ifelse( is.na(Bicarb),  impute.fx(joined.df$Bicarb, na.rm = T),  Bicarb)
    )
  
  # Threshold to create "WELL_MATCHED" column
  thresh.df <-
    impute.df %>%
    dplyr::mutate(
      MEAN_HGB = (NUM_VAL.x + NUM_VAL.y) / 2.,
      DIFF_HGB = abs(NUM_VAL.x - NUM_VAL.y),
      WELL_MATCHED = F # Default to FALSE
    )
  
  # Loop through the threshold list and OR together the WELL_MATCHED values
  for (t in thresh.list) {
    if (length(t) != 3)
      stop('Each vector within the threshold list must be three elements')
    
    thresh.df$WELL_MATCHED <-
      thresh.df$WELL_MATCHED | 
      (thresh.df$MEAN_HGB > t[1] & thresh.df$MEAN_HGB < t[2] & thresh.df$DIFF_HGB < t[3])
  }
  
  # Remove the MEAN and DIFF variables, as well as PN[1] value
  thresh.df <-
    thresh.df %>%
    dplyr::select(-MEAN_HGB, -DIFF_HGB, -NUM_VAL.x)
  

  cat(sprintf('Number (%%) of `WELL MATCHED`: %d (%0.2f %%)\n',
              sum(thresh.df$WELL_MATCHED),
              sum(thresh.df$WELL_MATCHED) / nrow(thresh.df) * 100.))
  
  # Run the logistic regression 
  reg.model <- glm(
    WELL_MATCHED ~ NUM_VAL.y +
      pH + 
      Gluc +
      Bicarb +
      iCal +
      Lactate +
      AGE_PROC + 
      DEPT,
    family = 'binomial',
    data = thresh.df %>%
      dplyr::mutate(pH = pH * 10.)
  )
  
  print(summary(reg.model))
  
  print(exp(reg.model$coefficients))
  
  if (ci) {
    
    ci.reg <- confint(reg.model)
    
    print(exp(ci.reg))
    
    return(list(
      reg.model,
      ci.reg
    ))
    
  } else {
    return(list(
      reg.model
    ))
  }
}
```

Now run the joining, imputing, and regression:

```{r}
# Threshold lists are a list of three-element vectors, which the three elements
# corresponding to: min, max, threshold
# This can be read as, between the min and max, the mean diff must be less than 
# the threshold, otherwise it is not `WELL_MATCHED`
thresh.list <-
  list(
    c(-100, 6, 1.5),
    c(6, 9, 1.0),
    c(9, 100, 1.5)
  )

regress.res <-
  joinImputeRegress(
    paired.df = cbc.bg,
    covars.df = covars.df,
    thresh.list = thresh.list,
    impute.fx = median,
    ci = F
  )
```

### Cohen's Kappa

Cohen's kappa coefficient provides an assessment of agreement between two "raters", or as an assessment of classification matching. In our case, we would like to understand the agreement between two PROC_NAME (e.g. CBC and BG) hemoglobins, when a simple threshold is applied. 

We hypothesize that there will be agreement at the tails but that there will be some disagreement in the local vicinity of the threshold, which will likely drive down the Kappa coefficient.

First, here is the function to compute the Cohen's Kappa:

```{r}
#'
#' @title Calculate Cohen Kappa
#'
#' @description Calculates the Cohen Kappa statistic for two vectors of CN values
#'
#' @details
#'     Recall that Cohen's Kappa is defined as:
#'
#'       K = (P_observed - P_expected) / (1 - P_expected)
#'
#'       Where:
#'         P_expected = sum(P_pos + P_neg) with
#'           P_pos = P_raterA+ x P_raterB+ and P_neg = P_raterA- x P_raterB-
#'
#'     In our case, Rater A will be positive when value.x (from PN[1]) are less
#'     than the cutoff, suggesting the need for a transfusion. Similarly, 
#'     Rater B will be positive when value.y (from PN[2]) are less than the
#'     cutoff.  
#'
#' A "Positive" response (meaning we have to transfuse) is when Hgb < cutoff,
#' and a "Negative" response (meaning we do not transfuse) is when Hgb >= cutoff.
#'
#' @param values.x A column vector of Hgb values from PN[1]
#' @param values.y A column vector of Hgb values from PN[2] (with length 
#'     same as rater.A.bg)
#' @param cutoff A scalar representing the Hgb cutoff value
#' @param to.print If TRUE, prints results in addition to returning [Default]
#'
#' @return The Cohen Kappa for these two vectors at the cutoff given
#'
calculateCohenKappa <- function (values.x, values.y, 
                                 cutoff = 7.0, to.print = T) {

  # Verify that the lengths of the two vectors of PN values are identical
  if (length(values.x) != length(values.y))
    stop('Error: vectors for X and Y must be of equal lengths')

  if (to.print)
    cat(sprintf('Pre-Range Check Length: %d\n', length(values.x)))
  
  if (to.print)
    cat(sprintf('Cutoff value used: %d\n', cutoff))

  # Consider a 2x2 matrix with two "Raters" (or two vectors):
  # 
  #                      values.x 
  #                   Yes   |     No
  #                -------------------
  #            Yes |   A    |    B   |
  #  values.y  ----|--------|--------|
  #            No  |   C    |    D   |
  #                -------------------
  #
  #  where: len = A + B + C + D
  #
  P.x.pos <- sum(values.x < cutoff) / length(values.x) # Equiv to 'A + C'  / len
  P.x.neg <- sum(values.x >= cutoff) / length(values.x) # Equiv to 'B + D' / len

  P.y.pos <- sum(values.y < cutoff) / length(values.y) # Equiv to 'A + B' / len
  P.y.neg <- sum(values.y >= cutoff) / length(values.y) # Equiv to ''C + D' / len

  if (to.print) {
    cat(sprintf('X :: Pos: %0.2f\tNeg: %0.2f\n', P.x.pos, P.x.neg))
    cat(sprintf('Y :: Pos: %0.2f\tNeg: %0.2f\n', P.y.pos, P.y.neg))
  }

  P.pos <- P.x.pos * P.y.pos
  P.neg <- P.x.neg * P.y.neg

  if (to.print)
    cat(sprintf('\tP.pos: %0.4f\n\tP.neg: %0.4f\n', P.pos, P.neg))

  P.exp <- P.pos + P.neg

  # Observed are the sum of counts of when both are either > or <=, divided by total
  # Note that we can divide by either length(rater.A.bg) or length(rater.B.cbc) 
  # since they are equal
  P.obs <- (
    sum((values.x < cutoff) & (values.y < cutoff)) +
      sum((values.x >= cutoff) & (values.y >= cutoff)) ) / length(values.x)

  if (to.print)
    cat(sprintf('\tP.obs: %0.2f\n\tP.exp: %0.2f\n', P.obs, P.exp))

  kappa <- (P.obs - P.exp) / (1. - P.exp)

  if (to.print)
    cat(sprintf('Kappa: %0.2f\n', kappa))

  return(kappa)
}
```

Now we use this function to calculate the Cohen's kappa at a given primary threshold:

```{r}
calculateCohenKappa(
  values.x = cbc.bg$NUM_VAL.x,
  values.y = cbc.bg$NUM_VAL.y,
  cutoff = primary.hgb.cutoff,
  to.print = T
)
```

And run again across the secondary thresholds:

```{r}
for (thresh in sens.hgb.cutoffs) {
  
  calculateCohenKappa(
    values.x = cbc.bg$NUM_VAL.x,
    values.y = cbc.bg$NUM_VAL.y,
    cutoff = thresh,
    to.print = T
  )
  
}

rm(thresh)
```

### Transfusion Tests

In this section, we ask the question, "If the BG Hgb value is greater than X, what is the likelihood that the CBC Hgb is less than Y?" for an appropriate transfusion threshold Y.

Similarly, we can ask the question, "If the BG Hgb value is less than X, what is the likelihood that the CBC Hgb is greater than Y?" for the same set of transfusion thresholds.

First we create the function to calculate the 2x2 matrix for this test:

```{r}
.lt <- function (a,b) { return (a < b) }
.lte <- function (a,b) { return (a <= b) }
.gt <- function (a,b) { return (a > b) }
.gte <- function (a,b) { return (a >= b) }

#'
#' @title Transfusion Confusion Matrix
#' 
#' @description Creates a 2x2 confusion matrix for a pair of cutoffs and direction
#' 
#' @param value.x
#' @param value.y
#' @param cutoffs A two-element vector which specifies the cutoffs for PN[1] 
#'     (which is typically the gold standard CBC) and for PN[2] (typically the
#'     "test" BG or iStat)
#' 
#'
transfusionConfusionMatrix <- function (value.x, value.y,
                                  cutoffs = c(7., 7.),
                                  direction = .lt,
                                  to.print = T) {

  # Are the lengths equal
  stopifnot(length(value.x) == length(value.y))

  # Is `cutoffs` a two-element vector?
  stopifnot(length(cutoffs) == 2)

  # Helper function to determine if two functions are identical
  sameDefinition <- function(x, y) { 

    stopifnot( is.function(x) )
    stopifnot( is.function(y) )

    identical(all.equal(x, y), TRUE)
  } 
  
  # Check the `direction` parameter
  if (sameDefinition(direction, .lt))
    opp.direction <- .gte
  else if (sameDefinition(direction, .gt))
    opp.direction <- .lte
  else
    stop('Direction function must either be `.lt` or `.gt` functions')
  
  if (to.print)
    cat(sprintf('Total number of input rows: %d\n',length(value.x)))

  # 2x2 Standard Table:
  #
  #               Gold Standard (PN[1])
  #                      value.x
  #                 Pos     |   Neg
  #               ----------------------
  #           Pos |   TP    |     FP   |
  # Test (PN[2])  |---------|----------|
  #  value.y  Neg |   FN    |     TN   |
  #               ----------------------
  #
  # In the default case, we consider "POS" to reflect the true need for a 
  # transfusion, meaning the value was < the cutoff
  
  if (to.print)
    cat(sprintf('Gold Standard:\n\tPositive: %d (%0.2f %%)\n\tNegative: %d (%0.2f %%)\n',
                sum( direction(value.x, cutoffs[1]) ),
                sum( direction(value.x, cutoffs[1]) ) / length(value.x) * 100.,
                sum( opp.direction(value.x, cutoffs[1]) ),
                sum( opp.direction(value.x, cutoffs[1]) ) / length(value.x) * 100.))
  
  # Rater A - yes, Rater B - yes
  TP <- sum( direction(value.x, cutoffs[1]) & direction(value.y, cutoffs[2]) )

  # Rater A - yes, Rater B - no
  FP <- sum( opp.direction(value.x, cutoffs[1]) & direction(value.y, cutoffs[2]) )

  # Rater A - no, Rater B - yes
  FN <- sum( direction(value.x, cutoffs[1]) & opp.direction(value.y, cutoffs[2]) )

  # Rater A - no, Rater B - no
  TN <- sum( opp.direction(value.x, cutoffs[1]) & opp.direction(value.y, cutoffs[2]) )

  # Dummy check - do these all add up to total length
  stopifnot(TP+FP+TN+FN == length(value.x))
  
  sens <- TP / (TP + FN)
  spec <- TN / (TN + FP)
  ppv <- TP / (TP + FP)
  npv <- TN / (FN + TN)

  if (to.print)
    cat(sprintf(paste0(
      'TP: %d (%0.4f %%)\t',
      'FP: %d (%0.4f %%)\n',
      'FN: %d (%0.4f %%)\t',
      'TN: %d (%0.4f %%)\n',
      'Sens: %0.4f\n',
      'Spec: %0.4f\n',
      'PPV: %0.4f\n',
      'NPV: %0.4f\n',
      'FOR (1-NPV): %0.4f\n',
      'NNM (1/FOR): %0.4f\n'),
      TP, TP / length(value.x) * 100.,
      FP, FP / length(value.x) * 100.,
      FN, FN / length(value.x) * 100.,
      TN, TN / length(value.x) * 100.,
      sens, spec, ppv, npv, (1. - npv), 1. / (1. - npv)
    ))
}
```

```{r}
transfusionConfusionMatrix(
  value.x = cbc.bg$NUM_VAL.x,
  value.y = cbc.bg$NUM_VAL.y,
  cutoffs = c(7., 7.),
  direction = .lt,
  to.print = T
)
```

### Save Analyses

Now we save out all of the plots and calculations that we have completed:

```{r}
save(
  file = file.path(
    Sys.getenv('PICU_LAB_DATA_PATH'),
    paste0(
      Sys.getenv('PICU_LAB_SITE_NAME'),
      '_pri_cbc_bg_clinical_',
      run.date, '.rData'
    )
  ),
  primary.cutoff,
  cbc.bg, covars.df, 
  error.grid.cbc.bg,
  thresh.list, regress.res,
  primary.hgb.cutoff
)
```

```{r, echo=FALSE}
rm(covars.df, cbc.bg, error.grid.cbc.bg, thresh.list, regress.res)
```
## CBC vs iSTAT

Here we re-do the clinical accuracy analyses across CBC vs POC (iStat) procedures. To do this, we first (again) define a function that runs all of the above work - similar to in `02_Analytic_Accuracy.Rmd`, except this function is termed `runAllClinical()`. 

```{r}
#'
#' @title Run All Clinical
#' 
#' @description ...
#'
runAllClinical <- function () {
  
  
  
}
```

## Sensitivity Analyses



